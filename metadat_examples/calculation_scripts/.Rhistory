df_sub %>% ggplot( aes(x = date_published_f, y = count, color = as.factor(category_minor))) +
geom_point(size = 1, alpha = 0.5) +
# Add a line based on a linear model for the people scoring less than 70
geom_smooth(data = filter(df_sub,  (date_published_f<44)), method = "lm") +
# Add a line based on a linear model for the people scoring 70 or more
geom_smooth(data = filter(df_sub, (date_published_f>=44)), method = "lm") +
geom_vline(xintercept = 44) +
facet_wrap(~category_f,scales="free") +
labs(x = "Entrance exam score", y = "Exit exam score", color = "Used tutoring")
df_sub <- df %>% filter(category_f >30 & category_f<40)
df_sub %>% ggplot( aes(x = date_published_f, y = count, color = as.factor(category_minor))) +
geom_point(size = 1, alpha = 0.5) +
# Add a line based on a linear model for the people scoring less than 70
geom_smooth(data = filter(df_sub,  (date_published_f<44)), method = "lm") +
# Add a line based on a linear model for the people scoring 70 or more
geom_smooth(data = filter(df_sub, (date_published_f>=44)), method = "lm") +
geom_vline(xintercept = 44) +
facet_wrap(~category_f,scales="free") +
labs(x = "Entrance exam score", y = "Exit exam score", color = "Used tutoring")
############
# simple DiD
didreg = lm(count ~ treated + after_treat + treated_group, data = df)
summary(didreg)
cat= df['category_minor'][0]
df_group <- df %>% filter(category_minor==cat)
df_group <- df %>% filter(category_minor== eval(cat))
df_group <- df %>% filter(category_minor== !eval(cat))
df_group <- df %>% filter(category_minor== !var(cat))
var(ccat)
vars(cat)
df_group <- df %>% filter(category_minor== eval(as.symbol(cat))))
df_group <- df %>% filter(category_minor== eval(as.symbol(cat)))
df_group <- df %>% filter(category_minor== eval(parse(cat)))
eval(parse(cat))
cat= df['category_minor'][0]
df_group <- df %>% filter(category_minor== eval(parse(cat)))
cat
cat= df['category_minor']
cat
cat= df['category_minor'][0]
cat
cat= df['category_minor'][1]
cat
cat= df['category_minor'][1,]
cat
df_group <- df %>% filter(category_minor== eval(parse(cat)))
eval(parse(cat))
parse(cat)
symbol(cat)
as.symbol(cat)
eval(as.symbol(cat))
df_group <- df %>% filter(category_minor== as.symbol(cat))
lm(count~after_treat,data=df_group)
df %>%
filter(category_f<5) %>%
ggplot(aes(x=as.Date(date_published),y=count)) +
geom_point() +
geom_vline(xintercept=as.Date("2022-11-30"),color='red')+
geom_smooth(method="lm", formula='y~x',geom='smooth') +
facet_wrap(~category_f,scales="free") +
theme_minimal()
df_sub %>% ggplot( aes(x = date_published_f, y = count, color = as.factor(treated_group))) +
geom_point(size = 1, alpha = 0.5) +
# Add a line based on a linear model for the people scoring less than 70
geom_smooth(data = filter(df_sub,  (date_published_f<44)), method = "lm") +
# Add a line based on a linear model for the people scoring 70 or more
geom_smooth(data = filter(df_sub, (date_published_f>=44)), method = "lm") +
geom_vline(xintercept = 44) +
facet_wrap(~category_f,scales="free") +
labs(x = "Entrance exam score", y = "Exit exam score", color = "Used tutoring")
nobs <- 500
tobs <- 6
iter <- 5000
mu_x <- 1
beta_x <- 1
sigma_u <- 1
did_staggered <- function(nobs, tobs) {
# Create a data frame with 'id' and expand it according to 'tobs'
data <- data.frame(id = rep(1:nobs, each = tobs))
# Create a 'year' variable
data$year <- rep(2001:(2000 + tobs), times = nobs)
# Create binary variables for each year
for (i in 1:tobs) {
data[, paste0("f0", sprintf("%02d", i))] <- as.integer(data$year == (2000 + i))
}
# Generate 't' as year - 2000
data$t <- data$year - 2000
# Generate 'c' with standard normal distribution and replicate per 'id'
set.seed(123) # For reproducibility
c_values <- rnorm(nobs)
data$c <- rep(c_values, each = tobs)
return(data)
}
# Usage
nobs <- 500
tobs <- 6
data_panel <- did_staggered(nobs, tobs)
data_panel
did_staggered <- function(nobs, tobs, sigma_u) {
# Setting the seed for reproducibility
set.seed(123)
# Create initial data frame
id <- rep(1:nobs, each = tobs)
year <- rep(2001:(2000 + tobs), times = nobs)
# Create binary variables for each year
data <- data.frame(id = id, year = year)
for (i in 1:tobs) {
data[[paste0("f0", i+1)]] <- as.integer(year == (2000 + i))
}
# Create other variables
data$t <- data$year - 2000
c_values <- rnorm(nobs)
data$c <- rep(c_values, each = tobs)
x0 <- rgamma(nobs * tobs, 1, 1)
data$x <- ave(x0, data$id, FUN = mean)
u <- rnorm(nobs * tobs, 0, sigma_u)
# Treatment cohorts
data$trt <- -0.5 + data$x/2 + rnorm(nobs * tobs) > 0
data$trt_sum <- ave(data$trt, data$id, FUN = sum)
data$dinf <- as.integer(data$trt_sum <= 2)
data$d4 <- as.integer(data$trt_sum == 3)
data$d5 <- as.integer(data$trt_sum == 4)
data$d6 <- as.integer(data$trt_sum >= 5)
# Drop unnecessary variables
data$trt <- NULL
data$trt_sum <- NULL
# Potential outcomes
yinfstar <- 2 + .2*data$f02 + .3*data$f03 + .4*data$f04 + .5*data$f05 + .6*data$f06 + data$x/5 - (data$d4 + data$d5 + data$d6) + data$c
data$yinf <- rpois(nobs * tobs, lambda = exp(yinfstar))
data$y4 <- with(data, ifelse(year >= 2004, rpois(nobs * tobs, lambda = exp(yinfstar + .4 + (x - 1)/5 + .4*f05 + .6*f06)), yinf))
data$y5 <- with(data, ifelse(year >= 2005, rpois(nobs * tobs, lambda = exp(yinfstar + .6 + (x - 1)/5 + .4*f06)), yinf))
data$y6 <- with(data, ifelse(year >= 2006, rpois(nobs * tobs, lambda = exp(yinfstar + .4 + (x - 1)/5)), yinf))
# More variables...
# (Continue creating variables and performing calculations as needed)
# ...
# Treatment effects
data$te_4 <- data$y4 - data$yinf
data$te_5 <- data$y5 - data$yinf
data$te_6 <- data$y6 - data$yinf
# Create time-varying treatment indicator for staggered intervention
data$w <- with(data, d4*(f04 + f05 + f06) + d5*(f05 + f06) + d6*f06)
# Calculate means for required groups to return
means <- list(
dinf_p = mean(data$dinf),
d4_p = mean(data$d4),
d5_p = mean(data$d5),
d6_p = mean(data$d6),
yinf_zero_p = mean(data$yinf == 0),
att_44 = mean(data$te_4[data$d4 & data$f04]),
att_45 = mean(data$te_4[data$d4 & data$f05]),
att_46 = mean(data$te_4[data$d4 & data$f06]),
att_55 = mean(data$te_5[data$d5 & data$f05]),
att_56 = mean(data$te_5[data$d5 & data$f06]),
att_66 = mean(data$te_6[data$d6 & data$f06])
)
return(list(data = data, means = means))
}
# Usage
nobs <- 500
tobs <- 6
sigma_u <- 1
results <- did_staggered(nobs, tobs, sigma_u)
# View results
results
results$data
subset_data <- subset(results$data, !w)
# Run the Poisson regression
model <- glm(y ~ f02 + f03 + f04 + f05 + f06 +
I(f02 * x) + I(f03 * x) + I(f04 * x) + I(f05 * x) + I(f06 * x) +
d4 + d5 + d6 + x + I(d4 * x) + I(d5 * x) + I(d6 * x),
family = poisson(link = "log"), data = subset_data)
# Summary of the model
summary(model)
# Run the Poisson regression
model <- glm(yinf ~ f02 + f03 + f04 + f05 + f06 +
I(f02 * x) + I(f03 * x) + I(f04 * x) + I(f05 * x) + I(f06 * x) +
d4 + d5 + d6 + x + I(d4 * x) + I(d5 * x) + I(d6 * x),
family = poisson(link = "log"), data = subset_data)
# Summary of the model
summary(model)
# get list of studies
file_list <- list.files(path = '/Users/htr365/Documents/Side_Projects/09_founding_lab/semester_project/meta-studies/metadat_examples/filter_descriptions', recursive = TRUE)
file_list <- gsub('.txt','',file_list)
# load dataset
dataset <- eval(as.symbol(study_name))
# get input json
input <- fromJSON(paste0('/Users/htr365/Documents/Side_Projects/09_founding_lab/semester_project/meta-studies/metadat_examples/filter_descriptions/',study_name,'.txt'))
input <- toJSON(input)
library(metadat)
library(metafor)
library(hash)
library(jsonlite)
library(dplyr)
source('utils.R')
check_filtering <- function(dataset,input){
# filter dataset
init_dim <- dim(dataset)
dataset <- try(filter_data(dataset,input))
if (inherits(dataset,'try-error')){
return('failed')
} else{
final_dim <- dim(dataset)
same_dim <- try(init_dim[1]!=final_dim[1])
if (inherits(same_dim, "try-error")) {
return('failed')}
else {
if (same_dim){
return('failed')
} else{return('success')}
}
}
}
# get list of studies
file_list <- list.files(path = '/Users/htr365/Documents/Side_Projects/09_founding_lab/semester_project/meta-studies/metadat_examples/filter_descriptions', recursive = TRUE)
file_list <- gsub('.txt','',file_list)
results <- c()
# load dataset
dataset <- eval(as.symbol(study_name))
# get input json
input <- fromJSON(paste0('/Users/htr365/Documents/Side_Projects/09_founding_lab/semester_project/meta-studies/metadat_examples/filter_descriptions/',study_name,'.txt'))
input <- toJSON(input)
input
study_name <- file_list[1]
# get input json
input <- fromJSON(paste0('/Users/htr365/Documents/Side_Projects/09_founding_lab/semester_project/meta-studies/metadat_examples/filter_descriptions/',study_name,'.txt'))
input <- toJSON(input)
input
out <- check_filtering(dataset,input)
# load dataset
dataset <- eval(as.symbol(study_name))
out <- check_filtering(dataset,input)
source('utils.R')
check_filtering <- function(dataset,input){
# filter dataset
init_dim <- dim(dataset)
dataset <- try(filter_data(dataset,input))
if (inherits(dataset,'try-error')){
return('failed')
} else{
final_dim <- dim(dataset)
same_dim <- try(init_dim[1]!=final_dim[1])
if (inherits(same_dim, "try-error")) {
return('failed')}
else {
if (same_dim){
return('failed')
} else{return('success')}
}
}
}
out <- check_filtering(dataset,input)
source('utils.R')
source('utils')
setwd('/Users/htr365/Documents/Side_Projects/09_founding_lab/semester_project/meta-studies/metadat_examples/calculation_scripts')
source('utils.R')
out <- check_filtering(dataset,input)
out
input
study_name
# test
study_name <- 'dat.aloe203'
dataset <- eval(as.symbol(study_name))
input <- fromJSON(paste0('/Users/htr365/Documents/Side_Projects/09_founding_lab/semester_project/meta-studies/metadat_examples/filter_descriptions/',study_name,'.txt'))
input <- toJSON(input)
input
calculate_meta(dataset,input)
calculate_meta <- function(dataset,input){
# filter dataset
dataset <- filter_data(dataset,input)
# run analysis
# get relevant function
function_name <- paste0('f.',str_split(study_name,'\\.')[[1]][2])
res <- eval(as.symbol(function_name))(dataset)
# format return table
out <- create_return(dataset,res)
}
calculate_meta(dataset,input)
# filter dataset
dataset <- filter_data(dataset,input)
input
input <- fromJSON(paste0('/Users/htr365/Documents/Side_Projects/09_founding_lab/semester_project/meta-studies/metadat_examples/filter_descriptions/',study_name,'.txt'))
input
# get input json
input <- fromJSON(paste0('/Users/htr365/Documents/Side_Projects/09_founding_lab/semester_project/meta-studies/metadat_examples/filter_descriptions/',study_name,'.txt'))
# get list of studies
file_list <- list.files(path = '/Users/htr365/Documents/Side_Projects/09_founding_lab/semester_project/meta-studies/metadat_examples/filter_descriptions', recursive = TRUE)
file_list <- gsub('.txt','',file_list)
file_list
study_name=file_list[1]
study_name
# load dataset
dataset <- eval(as.symbol(study_name))
# get input json
input <- fromJSON(paste0('/Users/htr365/Documents/Side_Projects/09_founding_lab/semester_project/meta-studies/metadat_examples/filter_descriptions/',study_name,'.txt'))
input
# test
study_name <- 'dat.aloe2013'
dataset <- eval(as.symbol(study_name))
input <- fromJSON(paste0('/Users/htr365/Documents/Side_Projects/09_founding_lab/semester_project/meta-studies/metadat_examples/filter_descriptions/',study_name,'.txt'))
input <- toJSON(input)
calculate_meta <- function(dataset,input){
# filter dataset
dataset <- filter_data(dataset,input)
# run analysis
# get relevant function
function_name <- paste0('f.',str_split(study_name,'\\.')[[1]][2])
res <- eval(as.symbol(function_name))(dataset)
# format return table
out <- create_return(dataset,res)
}
calculate_meta(dataset,input)
source('utils.R')
source('meta_analysis.R')
source('utils.R')
source('meta_analysis.R')
calculate_meta(dataset,input)
# filter dataset
dataset <- filter_data(dataset,input)
# run analysis
# get relevant function
function_name <- paste0('f.',str_split(study_name,'\\.')[[1]][2])
res <- eval(as.symbol(function_name))(dataset)
function_name
#create dataframe with study data
# check if dataset has year variable and if so how its written
if('year' %in% names(dataset)){
study_names <- paste0(dataset$study,', ', dataset$year)[res$not.na.yivi]
} else if ('Year' %in% names(dataset)){
study_names <- paste0(dataset$Study,', ', dataset$Year)[res$not.na.yivi]
} else {study_names <- dataset[1,]}
measure <- exp(res$yi)
participants <- attr(res$yi, 'ni')
ci_lower <- exp(res$yi -sqrt(res$vi)* qt(0.975, df = participants-1))
ci_upper <- exp(res$yi +sqrt(res$vi)* qt(0.975, df = participants-1))
out <- tibble(study_names, measure, participants, ci_lower, ci_upper)
# run analysis
# get relevant function
function_name <- paste0('f.',str_split(study_name,'\\.')[[1]][2])
res <- eval(as.symbol(function_name))(dataset)
function_name
dataset
# test
study_name <- 'dat.aloe2013'
dataset <- eval(as.symbol(study_name))
# filter dataset
dataset <- filter_data(dataset,input)
dataset
dat.aloe2013
source('meta_analysis.R')
res <- eval(as.symbol(function_name))(dataset)
dataset
### compute the partial correlation coefficients and corresponding sampling variances
dat <- escalc(measure="PCOR", ti=tval, ni=n, mi=preds, data=dataset)
res <-rma(yi, vi, data=dat)
source('meta_analysis.R')
res <- eval(as.symbol(function_name))(dataset)
# format return table
out <- create_return(dataset,res)
res
#create dataframe with study data
# check if dataset has year variable and if so how its written
if('year' %in% names(dataset)){
study_names <- paste0(dataset$study,', ', dataset$year)[res$not.na.yivi]
} else if ('Year' %in% names(dataset)){
study_names <- paste0(dataset$Study,', ', dataset$Year)[res$not.na.yivi]
} else {study_names <- dataset[1,]}
measure <- exp(res$yi)
measure
res$yi
res$yi
res
participants <- attr(res$yi, 'ni')
participants
ci_lower <- exp(res$yi -sqrt(res$vi)* qt(0.975, df = participants-1))
ci_upper <- exp(res$yi +sqrt(res$vi)* qt(0.975, df = participants-1))
ci_lower
res$yi
out <- tibble(study_names, measure, participants, ci_lower, ci_upper)
# add metastudy result
meta <- data.frame(study_names='Random Effects Model',
measure=exp(res$beta)[1],
participants = sum(participants),
ci_lower = exp(res$ci.lb),
ci_upper = exp(res$ci.ub))
# create json to export
out <- rbind(out,meta)
out
meta
study_names
#create dataframe with study data
# check if dataset has year variable and if so how its written
if('year' %in% names(dataset)){
study_names <- paste0(dataset$study,', ', dataset$year)[res$not.na.yivi]
} else if ('Year' %in% names(dataset)){
study_names <- paste0(dataset$Study,', ', dataset$Year)[res$not.na.yivi]
} else {study_names <- dataset[1,]}
study_names
dataset
dataset$study
dataset[1,]
dataset[,1]
#create dataframe with study data
# check if dataset has year variable and if so how its written
if('year' %in% names(dataset)){
study_names <- paste0(dataset$study,', ', dataset$year)[res$not.na.yivi]
} else if ('Year' %in% names(dataset)){
study_names <- paste0(dataset$Study,', ', dataset$Year)[res$not.na.yivi]
} else {study_names <- dataset[,1]}
measure <- exp(res$yi)
participants <- attr(res$yi, 'ni')
ci_lower <- exp(res$yi -sqrt(res$vi)* qt(0.975, df = participants-1))
ci_upper <- exp(res$yi +sqrt(res$vi)* qt(0.975, df = participants-1))
out <- tibble(study_names, measure, participants, ci_lower, ci_upper)
# add metastudy result
meta <- data.frame(study_names='Random Effects Model',
measure=exp(res$beta)[1],
participants = sum(participants),
ci_lower = exp(res$ci.lb),
ci_upper = exp(res$ci.ub))
# create json to export
out <- rbind(out,meta)
out <- out %>% mutate(across(where(is.numeric), \(x) round(x,digits=2)))
out_json <- toJSON(out,pretty=TRUE)
# gather study information
meta_info <- c(measure = unname(res$measure[[1]]),
call = paste0(trimws(deparse(res$call)),collapse=''),
method =res$method[[1]],
ll = res$fit.stats['ll',]['REML'],
dev = res$fit.stats['dev',]['REML'],
AIC = res$fit.stats['AIC',]['REML'],
BIC = res$fit.stats['BIC',]['REML'],
AICc = res$fit.stats['AICc',]['REML'],
pval = res$pval,
zval = res$zval,
n=res$k,
QE=res$QE)
source('utils.R')
source('utils.R')
# format return table
out <- create_return(dataset,res)
# test
#study_name <- 'dat.aloe2013'
#dataset <- eval(as.symbol(study_name))
input <- fromJSON(paste0('/Users/htr365/Documents/Side_Projects/09_founding_lab/semester_project/meta-studies/metadat_examples/filter_descriptions/',study_name,'.txt'))
input
input
#dataset <- eval(as.symbol(study_name))
input <- fromJSON(paste0('/Users/htr365/Documents/Side_Projects/09_founding_lab/semester_project/meta-studies/metadat_examples/filter_descriptions/',study_name,'.txt'))
#dataset <- eval(as.symbol(study_name))
input <- fromJSON(paste0('/Users/htr365/Documents/Side_Projects/09_founding_lab/semester_project/meta-studies/metadat_examples/filter_descriptions/',study_name,'.txt'))
#dataset <- eval(as.symbol(study_name))
input <- fromJSON(paste0('/Users/htr365/Documents/Side_Projects/09_founding_lab/semester_project/meta-studies/metadat_examples/filter_descriptions/',study_name,'.txt'))
input <- toJSON(input)
input
study_name
?dat.axfors2021
metadat
?metadat
help(packge=metadat)
help(package=metadat)
dataset <- dat.axfors2021
# calculate log odds ratios and corresponding sampling variances
dat <- escalc(measure="OR", ai=hcq_arm_event, n1i=hcq_arm_total,
ci=control_arm_event, n2i=control_arm_total, data=dataset)
dat
res <- rma(yi, vi,  slab = id, data=dat)
res
dataset M- dat.bakdash2021
dataset <- dat.bakdash2021
### multilevel meta-analytic model to get the overall pooled effect
res <- rma.mv(es.z, vi.z, mods = ~ 1,
random = ~ 1 | SampleID / Outcome,
data = dataset,
test = "t")
res
dataset <- dat.baker2009
### Transform data from long arm-based format to contrast-based
### format. Argument 'sm' has to be used for odds ratio as summary
### measure; by default the risk ratio is used in the metabin function
### called internally.
pw <- pairwise(treatment, exac, total, studlab = paste(study, year),
data = dataset, sm = "OR")
library(netmeta)
### Transform data from long arm-based format to contrast-based
### format. Argument 'sm' has to be used for odds ratio as summary
### measure; by default the risk ratio is used in the metabin function
### called internally.
pw <- pairwise(treatment, exac, total, studlab = paste(study, year),
data = dataset, sm = "OR")
### Conduct random effects network meta-analysis (NMA)
### with placebo as reference
net <- netmeta(pw, fixed = FALSE, ref = "plac")
net
res
create_return(dataset,net)
typof(res)
typeof(res)
typeof(net)
net
res <- net
#create dataframe with study data
# check if dataset has year variable and if so how its written
if('year' %in% names(dataset)){
study_names <- paste0(dataset$study,', ', dataset$year)[res$not.na.yivi]
} else if ('Year' %in% names(dataset)){
study_names <- paste0(dataset$Study,', ', dataset$Year)[res$not.na.yivi]
} else {study_names <- dataset[,1]}
measure <- exp(res$yi)
participants <- attr(res$yi, 'ni')
ci_lower <- exp(res$yi -sqrt(res$vi)* qt(0.975, df = participants-1))
ci_upper <- exp(res$yi +sqrt(res$vi)* qt(0.975, df = participants-1))
res$yi
